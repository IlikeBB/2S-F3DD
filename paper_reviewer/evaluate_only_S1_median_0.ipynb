{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Segmentation Models: using `keras` framework.\n"
     ]
    }
   ],
   "source": [
    "# %env SM_FRAMEWORK=tf.keras\n",
    "import zipfile, os, numpy as np, pickle, yaml, gc, tensorflow as tf\n",
    "import segmentation_models as sm\n",
    "import tensorflow_addons as tfa\n",
    "sys.path.append(\"..\")\n",
    "from keras import backend as K\n",
    "K.clear_session()\n",
    "from sklearn.metrics import jaccard_score\n",
    "from segmentation_models import Unet\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "os.environ['TF_FORCE_GPU_ALLOW_GROWTH'] = \"true\"\n",
    "class_type = 0 # 0=NL, 1=AP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 1], dtype=int8)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array([0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1.0]).astype(np.int8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path test\n",
    "top_layer_path_train = '../my_data/datasets/all_seg_data_NL_zm_2/'\n",
    "top_layer_path = '../my_data/datasets/test_data_NL/'\n",
    "# S1 [img, msk]\n",
    "S1_img_stack = ['T3_image_arr_384_valid.npy', 'T3_masks_arr_384_valid.npy', 'T3_image_arr_384_train.npy', 'T3_masks_arr_384_train.npy']\n",
    "# save path\n",
    "# save_path = f'results_log/S1/all_S1-test_results_{datatype}'\n",
    "\n",
    "# S1 model weight path\n",
    "S1_weights=['../my_data/my_weights/S1_DenseNet.hdf5','../my_data/my_weights/S1_ResNet.hdf5','../my_data/my_weights/S1_VGGNet.hdf5']\n",
    "# S1_weights=['./checkpoints/S1-segment/best-valid-auc_2DDenseUnet-12.16-original_3T_NL-data_dice-loss.hdf5', './checkpoints/S1-segment/best-valid-auc_2DResNet50-12.16-original_3T_NL-data_dice-loss.hdf5', config[\"S1_vgg\"]]\n",
    "S1_backbone = ['densenet121', 'resnet50', 'vgg16']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 1 0 0 0 1 1 0 1 1 1 1 1 1 0 0 1 1 0 0 1 0 1 0 0 0 0 0 1 0 0 1 0 0 0\n",
      " 1 0 0 0 0 1 0 0]\n",
      "[0 1 0 0 0 0 1 0 1 1 0 0 0 1 0 1 0 1 1 0 0 0 1 0 1 0 1 0 1 1 0 1 1 0 1 0 0\n",
      " 1 0 0 0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "compare_csv = pd.read_csv('../paper_reviewer/Only Test data.csv')\n",
    "# print(compare_csv.columns)\n",
    "fiter_csv = np.array(compare_csv[['Test MRI', 'Mean Image Intensity', 'Mask Pixel Sum']])\n",
    "mri_value = np.array([i for i in fiter_csv if 'is' in i[0]])\n",
    "# print(mri_value)\n",
    "mri_compare_value = np.array([i for i in fiter_csv if ('is' not in i[0] and 'overall' in i[0])])\n",
    "# print(mri_compare_value)\n",
    "mri_value_filter_intensity = np.array([1 if i[1]>mri_compare_value[3][1] else 0 for i in mri_value ])\n",
    "print(mri_value_filter_intensity)\n",
    "mri_value_filter_pixel = np.array([1 if i[2]>mri_compare_value[3][2] else 0 for i in mri_value ])\n",
    "print(mri_value_filter_pixel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def S1_dataloader(valid_data='Mix', tune_type='test', type_data=None):\n",
    "    if tune_type=='test':\n",
    "        img_layer_path = top_layer_path\n",
    "        if valid_data == '3.0T':\n",
    "            # loading valida data 3.0T + 1.5T: image / masks\n",
    "            X_valid = np.load(img_layer_path +'/'+ S1_img_stack[0])\n",
    "            y_valid = np.load(img_layer_path +'/'+ S1_img_stack[1])\n",
    "    elif tune_type=='train':\n",
    "        img_layer_path = top_layer_path_train\n",
    "        X_valid = np.load(img_layer_path +'/'+ S1_img_stack[2])\n",
    "        y_valid = np.load(img_layer_path +'/'+ S1_img_stack[3])\n",
    "    # print(len(experiment_stack[type_data]))\n",
    "    # print(np.where(experiment_stack[type_data]==1))\n",
    "    \n",
    "    if \">\" in type_data:\n",
    "        X_valid =  X_valid[np.where(experiment_stack[type_data]==1)]\n",
    "        y_valid = y_valid[np.where(experiment_stack[type_data]==1)]\n",
    "    else:\n",
    "        # print(np.where(experiment_stack[type_data]==0))\n",
    "        X_valid = X_valid[np.where(experiment_stack[type_data]==0)]\n",
    "        y_valid = y_valid[np.where(experiment_stack[type_data]==0)]\n",
    "    print(X_valid.shape)\n",
    "    X_valid = np.reshape(X_valid, (X_valid.shape[0]*32,384,384,1))\n",
    "    y_valid = np.reshape(y_valid, (y_valid.shape[0]*32,384,384,1))\n",
    "    return X_valid.astype(np.float32), y_valid.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def S1_model_loader(weight_path, backbone, mode, type_data=None):\n",
    "    S1_X_valid, S1_y_valid= S1_dataloader(valid_data='3.0T', tune_type=mode, type_data=type_data)\n",
    "    print(f'S1 data shape: img {S1_X_valid.shape} msk {S1_y_valid.shape}')\n",
    "    model = Unet(backbone, encoder_weights=None, input_shape=(None, None, 1))\n",
    "    model.load_weights(weight_path)\n",
    "    Results = model.predict(S1_X_valid, batch_size=1, verbose=1)\n",
    "    return np.array(Results), np.array(S1_X_valid), np.array(S1_y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "S1_thresholds = 0.5\n",
    "S1_pred_stack=[]\n",
    "class metric_:\n",
    "    def __init__(self,y_true, y_pred):\n",
    "        self.y_true_f = K.flatten(y_true)\n",
    "        self.y_pred_f = K.flatten(y_pred)\n",
    "        self.current = confusion_matrix(K.cast(self.y_true_f, dtype='int8'), K.cast(self.y_pred_f, dtype='int8'), labels=[0, 1])\n",
    "    def dice_coef(self):\n",
    "        # intersection = K.sum(self.y_true_f* self.y_pred_f )\n",
    "        intersection = np.diag(self.current)\n",
    "        ground_truth_set = self.current.sum(axis=1)\n",
    "        predicted_set = self.current.sum(axis=0)\n",
    "        # dice = (2. * intersection) / (K.sum(self.y_true_f) + K.sum(self.y_pred_f ))\n",
    "        dice = (2. * intersection) / (ground_truth_set + predicted_set)\n",
    "        return np.mean(dice)\n",
    "        \n",
    "    def iou(self):\n",
    "        # ytrue, ypred is a flatten vector\n",
    "        # compute mean iou\n",
    "        intersection = np.diag(self.current)\n",
    "        ground_truth_set = self.current.sum(axis=1)\n",
    "        predicted_set = self.current.sum(axis=0)\n",
    "        union = ground_truth_set + predicted_set - intersection\n",
    "        IoU = intersection / union.astype(np.float32)\n",
    "        return np.mean(IoU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start model = test\n",
      "----------Mean Intensity < Overall Mean-------------↓ \n",
      "(28, 32, 384, 384)\n",
      "S1 data shape: img (896, 384, 384, 1) msk (896, 384, 384, 1)\n",
      "896/896 [==============================] - 29s 33ms/step\n",
      "Dice:  0.8940396846904755\n",
      "IoU:   0.8250696920089773\n",
      "----------Mean Intensity > Overall Mean-------------↓ \n",
      "(17, 32, 384, 384)\n",
      "S1 data shape: img (544, 384, 384, 1) msk (544, 384, 384, 1)\n",
      "544/544 [==============================] - 17s 31ms/step\n",
      "Dice:  0.9190784403555322\n",
      "IoU:   0.8606600305236347\n",
      "----------Sum Pixel < Overall Mean-------------↓ \n",
      "(28, 32, 384, 384)\n",
      "S1 data shape: img (896, 384, 384, 1) msk (896, 384, 384, 1)\n",
      "896/896 [==============================] - 28s 31ms/step\n",
      "Dice:  0.8785297871706145\n",
      "IoU:   0.8045362148305577\n",
      "----------Mean Pixel > Overall Mean-------------↓ \n",
      "(17, 32, 384, 384)\n",
      "S1 data shape: img (544, 384, 384, 1) msk (544, 384, 384, 1)\n",
      "544/544 [==============================] - 16s 29ms/step\n",
      "Dice:  0.9048207818705796\n",
      "IoU:   0.8399449721266359\n"
     ]
    }
   ],
   "source": [
    "# ----S1 model test 1 - 3----\n",
    "# mode_list=['test', 'train']\n",
    "experiment_stack = {'Mean Intensity < Overall Mean': mri_value_filter_intensity, 'Mean Intensity > Overall Mean': mri_value_filter_intensity, 'Sum Pixel < Overall Mean': mri_value_filter_pixel , 'Sum Pixel > Overall Mean': mri_value_filter_pixel}\n",
    "mode_list=['test']\n",
    "for idx,d in enumerate(mode_list):\n",
    "    print(f'Start model = {d}')\n",
    "    for i in experiment_stack:\n",
    "        print(f'----------{i}-------------↓ ')\n",
    "        S1_pred, S1_X_valid, S1_y_valid = S1_model_loader(S1_weights[1], S1_backbone[1], d,type_data= i)\n",
    "        metric_class = metric_(S1_y_valid,S1_pred)\n",
    "        print(\"Dice: \", metric_class.dice_coef())\n",
    "        print(\"IoU:  \", metric_class.iou())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4665f7484a0bffeb453a5c4a3f9172c008eba4e225e0a746b21d144b5d5fbcdb"
  },
  "kernelspec": {
   "display_name": "Python 3.7.9 64-bit ('SGD': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "metadata": {
   "interpreter": {
    "hash": "5d33ac8ce81c8da01985ba1576671d92ac976b7a4615b42eb65ecef4f329b894"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
