{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Segmentation Models: using `keras` framework.\n"
     ]
    }
   ],
   "source": [
    "# %env SM_FRAMEWORK=tf.keras\n",
    "import zipfile, os, numpy as np, pickle, yaml, gc, tensorflow as tf\n",
    "import segmentation_models as sm\n",
    "import tensorflow_addons as tfa\n",
    "sys.path.append(\"..\")\n",
    "from keras import backend as K\n",
    "K.clear_session()\n",
    "from sklearn.metrics import jaccard_score\n",
    "from segmentation_models import Unet\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "class_type = 0 # 0=NL, 1=AP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path test\n",
    "top_layer_path_train = '../my_data/datasets/all_seg_data_NL_zm_2/'\n",
    "top_layer_path = '../my_data/datasets/test_data_NL/'\n",
    "# S1 [img, msk]\n",
    "S1_img_stack = ['T3_image_arr_384_valid.npy', 'T3_masks_arr_384_valid.npy', 'T3_image_arr_384_train.npy', 'T3_masks_arr_384_train.npy']\n",
    "# save path\n",
    "# save_path = f'results_log/S1/all_S1-test_results_{datatype}'\n",
    "\n",
    "\n",
    "# S1 model weight path\n",
    "S1_weights=['../checkpoints/S1-segment/2DResNet50-384-epochs_100-lr_0.001-batch_8-2022.04.07-original_3T_NL-data_FTL-20--sample.hdf5',\n",
    "                            '../checkpoints/S1-segment/2DResNet50-384-epochs_100-lr_0.001-batch_8-2022.04.07-original_3T_NL-data_FTL-50--sample.hdf5',\n",
    "                            '../checkpoints/S1-segment/2DResNet50-384-epochs_100-lr_0.001-batch_8-2022.04.07-original_3T_NL-data_FTL-100--sample.hdf5',\n",
    "                            '../my_data/my_weights/S1_ResNet.hdf5',\n",
    "                            '../my_data/my_weights/S1_DenseNet.hdf5',\n",
    "                            '../my_data/my_weights/S1_VGGNet.hdf5']\n",
    "# S1_weights=['./checkpoints/S1-segment/best-valid-auc_2DDenseUnet-12.16-original_3T_NL-data_dice-loss.hdf5', './checkpoints/S1-segment/best-valid-auc_2DResNet50-12.16-original_3T_NL-data_dice-loss.hdf5', config[\"S1_vgg\"]]\n",
    "S1_backbone = ['resnet50', 'resnet50', 'resnet50', 'resnet50','densenet121','vgg16']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def S1_dataloader(valid_data='Mix', tune_type='test', data_len=None):\n",
    "    if tune_type=='test':\n",
    "        img_layer_path = top_layer_path\n",
    "        if valid_data == '3.0T':\n",
    "            # loading valida data 3.0T + 1.5T: image / masks\n",
    "            X_valid = np.load(img_layer_path +'/'+ S1_img_stack[0])\n",
    "            y_valid = np.load(img_layer_path +'/'+ S1_img_stack[1])\n",
    "    elif tune_type=='train':\n",
    "        img_layer_path = top_layer_path_train\n",
    "        if data_len == 999:\n",
    "            X_valid = np.load(img_layer_path +'/'+ S1_img_stack[2])\n",
    "            y_valid = np.load(img_layer_path +'/'+ S1_img_stack[3])\n",
    "        else:\n",
    "            X_valid = np.load(img_layer_path +'/'+ S1_img_stack[2])[0: data_len]\n",
    "            y_valid = np.load(img_layer_path +'/'+ S1_img_stack[3])[0: data_len]\n",
    "        \n",
    "    X_valid = np.reshape(X_valid, (X_valid.shape[0]*32,384,384,1))\n",
    "    y_valid = np.reshape(y_valid, (y_valid.shape[0]*32,384,384,1))\n",
    "    return X_valid.astype(np.float32), y_valid.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def S1_model_loader(weight_path, backbone, mode, data_len=None):\n",
    "    S1_X_valid, S1_y_valid= S1_dataloader(valid_data='3.0T', tune_type=mode, data_len=data_len)\n",
    "    print(f'S1 data shape: img {S1_X_valid.shape} msk {S1_y_valid.shape}')\n",
    "    model = Unet(backbone, encoder_weights=None, input_shape=(None, None, 1))\n",
    "    model.load_weights(weight_path)\n",
    "    Results = model.predict(S1_X_valid, batch_size=1, verbose=1)\n",
    "    return np.array(Results), np.array(S1_X_valid), np.array(S1_y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "S1_thresholds = 0.5\n",
    "S1_pred_stack=[]\n",
    "def dice_coef(y_true, y_pred):\n",
    "    y_true_f = K.flatten(y_true)\n",
    "    y_pred_f = K.flatten(y_pred)\n",
    "    intersection = K.sum(y_true_f* y_pred_f)\n",
    "    dice = (2. * intersection) / (K.sum(y_true_f) + K.sum(y_pred_f))\n",
    "    return dice\n",
    "def iou(t, p):\n",
    "    # ytrue, ypred is a flatten vector\n",
    "    y_pred = t.flatten()\n",
    "    y_true = p.flatten()\n",
    "    current = confusion_matrix(y_true.astype(np.int8), y_pred.astype(np.int8), labels=[0, 1])\n",
    "    # compute mean iou\n",
    "    intersection = np.diag(current)\n",
    "    ground_truth_set = current.sum(axis=1)\n",
    "    predicted_set = current.sum(axis=0)\n",
    "    union = ground_truth_set + predicted_set - intersection\n",
    "    IoU = intersection / union.astype(np.float32)\n",
    "    return np.mean(IoU)\n",
    "\n",
    "class metric_:\n",
    "    def __init__(self,y_true, y_pred):\n",
    "        self.y_true_f = K.flatten(y_true)\n",
    "        self.y_pred_f = K.flatten(y_pred)\n",
    "        # self.current = confusion_matrix(K.cast(self.y_true_f, dtype='int8'), K.cast((self.y_pred_f>0.5), dtype='int8'), labels=[0, 1])\n",
    "        self.tn, self.fp, self.fn, self.tp = confusion_matrix(K.cast(self.y_true_f, dtype='int8'), K.cast((self.y_pred_f>0.5), dtype='int8'), labels=[0, 1]).ravel()\n",
    "        # self.current = (tn, fp, fn, tp)\n",
    "    def dice_coef(self):\n",
    "        # intersection = K.sum(self.y_true_f* self.y_pred_f )\n",
    "        # intersection = np.diag(self.current)\n",
    "        # ground_truth_set = self.current.sum(axis=1)\n",
    "        # predicted_set = self.current.sum(axis=0)\n",
    "        # # dice = (2. * intersection) / (K.sum(self.y_true_f) + K.sum(self.y_pred_f ))\n",
    "        # dice = (2. * intersection) / (ground_truth_set + predicted_set)\n",
    "        dice = (2.*self.tp) / (self.tp + self.fn + self.tp + self.fp)\n",
    "        # return np.mean(dice)\n",
    "        return dice\n",
    "        \n",
    "    def iou(self):\n",
    "        # ytrue, ypred is a flatten vector\n",
    "        # compute mean iou\n",
    "        # intersection = np.diag(self.current)\n",
    "        # ground_truth_set = self.current.sum(axis=1)\n",
    "        # predicted_set = self.current.sum(axis=0)\n",
    "        # union = ground_truth_set + predicted_set - intersection\n",
    "        # IoU = intersection / union.astype(np.float32)\n",
    "        IoU = self.tp / (self.tp + self.fn + self.fp)\n",
    "        # return np.mean(IoU)\n",
    "        return IoU\n",
    "    def acc(self):\n",
    "        # tn, fp, fn, tp = self.current .ravel()\n",
    "        # return ((tp+tn)/(tp+fp+fn+tn))\n",
    "        return ((self.tp+self.tn)/(self.tp+self.fp+self.fn+self.tn))\n",
    "        # \"Accuracy: \"+str(round((tp+tn)/(tp+fp+fn+tn), 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start model = test\n",
      "Train Sample = 20 - resnet50\n",
      "S1 data shape: img (1440, 384, 384, 1) msk (1440, 384, 384, 1)\n",
      "1440/1440 [==============================] - 45s 31ms/step\n",
      "Dice:  0.6684784099073653\n",
      "IoU:   0.502040984450623\n",
      "Acc:   0.9994306587878569\n",
      "Train Sample = 50 - resnet50\n",
      "S1 data shape: img (1440, 384, 384, 1) msk (1440, 384, 384, 1)\n",
      "1440/1440 [==============================] - 43s 30ms/step\n",
      "Dice:  0.7752051030470006\n",
      "IoU:   0.6329264638312324\n",
      "Acc:   0.999567323849525\n",
      "Train Sample = 100 - resnet50\n",
      "S1 data shape: img (1440, 384, 384, 1) msk (1440, 384, 384, 1)\n",
      "1440/1440 [==============================] - 43s 30ms/step\n",
      "Dice:  0.7827925442234469\n",
      "IoU:   0.6431052821016787\n",
      "Acc:   0.9995843910876615\n",
      "Train Sample = 140 - resnet50\n",
      "S1 data shape: img (1440, 384, 384, 1) msk (1440, 384, 384, 1)\n",
      "1440/1440 [==============================] - 42s 29ms/step\n",
      "Dice:  0.8281570127352231\n",
      "IoU:   0.7067132898650882\n",
      "Acc:   0.9996322490550854\n",
      "Train Sample = 140 - densenet121\n",
      "S1 data shape: img (1440, 384, 384, 1) msk (1440, 384, 384, 1)\n",
      "1440/1440 [==============================] - 67s 46ms/step\n",
      "Dice:  0.8129242254958312\n",
      "IoU:   0.6848124129526463\n",
      "Acc:   0.999590739497432\n",
      "Train Sample = 140 - vgg16\n",
      "S1 data shape: img (1440, 384, 384, 1) msk (1440, 384, 384, 1)\n",
      "1440/1440 [==============================] - 30s 21ms/step\n",
      "Dice:  0.8056688680589964\n",
      "IoU:   0.6745774655891613\n",
      "Acc:   0.9995954866762514\n"
     ]
    }
   ],
   "source": [
    "# ----S1 model test 1 - 3----\n",
    "mode_list=['test']\n",
    "train_data_len = [20,50,100,999,999,999]\n",
    "for idx,d in enumerate(mode_list):\n",
    "    print(f'Start model = {d}')\n",
    "    for i in range(6):\n",
    "        if train_data_len[i]==999:\n",
    "            print(f'Train Sample = 140 - {S1_backbone[i]}')\n",
    "        else:\n",
    "            print(f'Train Sample = {train_data_len[i]} - {S1_backbone[i]}')\n",
    "        if d == 'train':\n",
    "            S1_pred, S1_X_valid, S1_y_valid = S1_model_loader(S1_weights[i], S1_backbone[i], d, data_len=train_data_len[i])\n",
    "        else:\n",
    "            S1_pred, S1_X_valid, S1_y_valid = S1_model_loader(S1_weights[i], S1_backbone[i], d)\n",
    "        metric_class = metric_(S1_y_valid,S1_pred)\n",
    "        print(\"Dice: \", metric_class.dice_coef())\n",
    "        print(\"IoU:  \", metric_class.iou())\n",
    "        print(\"Acc:  \", metric_class.acc())\n",
    "        # S1_pred_stack.append([S1_pred, S1_y_valid])\n",
    "        # print(\"Dice: \", dice_coef(S1_y_valid,S1_pred).numpy())\n",
    "        # print(\"IoU:  \", iou(S1_y_valid,S1_pred))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4665f7484a0bffeb453a5c4a3f9172c008eba4e225e0a746b21d144b5d5fbcdb"
  },
  "kernelspec": {
   "display_name": "Python 3.7.9 64-bit ('SGD': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "metadata": {
   "interpreter": {
    "hash": "5d33ac8ce81c8da01985ba1576671d92ac976b7a4615b42eb65ecef4f329b894"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
